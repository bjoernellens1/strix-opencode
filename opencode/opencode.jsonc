{
  "$schema": "https://opencode.ai/config.json",

  // Norse Agent Architecture ‚Äî Phase 5 Hybrid
  //
  // llama.cpp GGUF Q4_K_M (ROCm GPU): Thor, Valkyrie, Odin
  // vLLM BF16 (GPU): Heimdall, Loki, Frigga
  //
  // Model names differ by backend:
  //   llama.cpp: serves the GGUF filename (e.g., "qwen2.5-14b-instruct-q4_k_m.gguf")
  //   vLLM: serves the HuggingFace model ID (e.g., "Qwen/Qwen2.5-3B-Instruct")

  "plugin": [
    "./oh-my-opencode"
  ],

  "provider": {
    // ===== llama.cpp agents (GGUF) =====
    "local_thor": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Thor ‚ö° ‚Äî Primary Commander (llama.cpp, Qwen2.5-14B GGUF)",
      "options": { "baseURL": "http://127.0.0.1:8001/v1", "apiKey": "EMPTY" },
      "models": {
        // llama-server serves the GGUF filename as model name
        "thor": { "name": "qwen2.5-14b-instruct-q4_k_m.gguf", "tools": true }
      }
    },
    "local_valkyrie": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Valkyrie üõ° ‚Äî Execution Specialist (llama.cpp, Qwen3-Coder-30B GGUF)",
      "options": { "baseURL": "http://127.0.0.1:8002/v1", "apiKey": "EMPTY" },
      "models": {
        "valkyrie": { "name": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf", "tools": true }
      }
    },
    "local_odin": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Odin üëÅÔ∏è ‚Äî Supreme Architect (llama.cpp, Llama-3.3-70B GGUF)",
      "options": { "baseURL": "http://127.0.0.1:8011/v1", "apiKey": "EMPTY" },
      "models": {
        "odin": { "name": "llama-3.3-70b-instruct-Q4_K_M.gguf", "tools": true }
      }
    },
    // ===== vLLM agents (BF16) =====
    "local_heimdall": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Heimdall üëÅ ‚Äî Guardian (vLLM BF16, Qwen2.5-3B)",
      "options": { "baseURL": "http://127.0.0.1:8012/v1", "apiKey": "EMPTY" },
      "models": {
        "heimdall": { "name": "Qwen/Qwen2.5-3B-Instruct", "tools": true }
      }
    },
    "local_loki": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Loki üß† ‚Äî Adversarial Intelligence (vLLM BF16, Qwen2.5-7B)",
      "options": { "baseURL": "http://127.0.0.1:8013/v1", "apiKey": "EMPTY" },
      "models": {
        "loki": { "name": "Qwen/Qwen2.5-7B-Instruct", "tools": true }
      }
    },
    "local_frigga": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Frigga üåø ‚Äî Knowledge Curator (vLLM BF16, Qwen2.5-14B)",
      "options": { "baseURL": "http://127.0.0.1:8014/v1", "apiKey": "EMPTY" },
      "models": {
        "frigga": { "name": "Qwen/Qwen2.5-14B-Instruct", "tools": true }
      }
    },
    // ===== Cloud fallback =====
    "cloud_planner": {
      "npm": "@ai-sdk/openai",
      "name": "Cloud Planner",
      "options": { "apiKey": "${OPENAI_API_KEY}" },
      "models": { "planner": { "name": "${CLOUD_PLANNER_MODEL}", "tools": true } }
    }
  },

  "agent": {
    "primary":  { "mode": "primary",  "model": "local_thor:thor" },
    "coder":    { "mode": "subagent", "model": "local_valkyrie:valkyrie" },
    "reviewer": { "mode": "subagent", "model": "local_odin:odin" },
    "utility":  { "mode": "subagent", "model": "local_heimdall:heimdall" },
    "loki":     { "mode": "subagent", "model": "local_loki:loki" },
    "frigga":   { "mode": "subagent", "model": "local_frigga:frigga" }
  }
}
