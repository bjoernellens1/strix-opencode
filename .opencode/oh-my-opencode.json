{
  "$schema": "https://oh-my-opencode.dev/config.json",

  "_comment": [
    "Template: recommended maxTokens for local models on Strix Halo (Norse agent architecture).",
    "Copy this file into your target project's .opencode/ directory.",
    "",
    "Phase 5 — Hybrid Architecture (llama.cpp GGUF + vLLM BF16):",
    "",
    "  llama.cpp agents (GGUF Q4_K_M, ROCm GPU):",
    "    Thor (Qwen2.5-14B, ~8 GB): 64K context → 32K output + ~32K input",
    "    Valkyrie (Qwen3-Coder-30B, ~17 GB): 48K context → 24K output + ~24K input",
    "    Odin (Llama-3.3-70B, ~40 GB): 32K context → 16K output + ~16K input",
    "",
    "  vLLM agents (BF16):",
    "    Heimdall (Qwen2.5-3B, ~6 GB): 8K context → 4K output + ~4K input",
    "    Loki (Qwen2.5-7B, ~14 GB): 16K context → 8K output + ~8K input",
    "    Frigga (Qwen2.5-14B, ~28 GB): 16K context → 8K output + ~8K input",
    "",
    "Agent → tier mapping:",
    "  sisyphus (primary)           → Thor (llama.cpp, 64K)",
    "  hephaestus, sisyphus-junior  → Valkyrie (llama.cpp, 48K)",
    "  oracle, prometheus           → Odin (llama.cpp, 32K)",
    "  metis, momus                 → Frigga (vLLM, 16K)",
    "  librarian, explore, atlas    → Heimdall (vLLM, 8K)",
    "",
    "If you switch to cloud models, remove or raise these limits."
  ],

  "agents": {
    "sisyphus": {
      "maxTokens": 32768
    },
    "hephaestus": {
      "maxTokens": 24576
    },
    "sisyphus-junior": {
      "maxTokens": 24576
    },
    "oracle": {
      "maxTokens": 16384
    },
    "prometheus": {
      "maxTokens": 16384
    },
    "metis": {
      "maxTokens": 8192
    },
    "momus": {
      "maxTokens": 8192
    },
    "librarian": {
      "maxTokens": 4096
    },
    "explore": {
      "maxTokens": 4096
    },
    "atlas": {
      "maxTokens": 4096
    }
  }
}
