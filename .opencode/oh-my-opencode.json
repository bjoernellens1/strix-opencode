{
  "$schema": "https://oh-my-opencode.dev/config.json",

  "_comment": [
    "Template: recommended maxTokens for local models on Strix Halo (Norse agent architecture).",
    "Copy this file into your target project's .opencode/ directory.",
    "",
    "Thor (Qwen2.5-14B, GPU): 64K context → 32K output leaves ~32K for input.",
    "Valkyrie (Qwen3-Coder-30B-A3B, GPU): 48K context → 24K output leaves ~24K for input.",
    "Odin (Llama-3.3-70B GGUF, CPU): 32K context → 16K output leaves ~16K for input.",
    "Frigga (Qwen2.5-14B GGUF, CPU): 32K context → 16K output leaves ~16K for input.",
    "Loki (Qwen2.5-7B GGUF, CPU): 16K context → 8K output leaves ~8K for input.",
    "Heimdall (Qwen2.5-3B GGUF, CPU): 8K context → 4K output leaves ~4K for input.",
    "",
    "Agent → tier mapping:",
    "  sisyphus (primary)           → Thor (GPU, 64K)",
    "  hephaestus, sisyphus-junior  → Valkyrie (GPU, 48K)",
    "  oracle, prometheus           → Odin (CPU, 32K)",
    "  metis, momus                 → Frigga (CPU, 32K)",
    "  librarian, explore, atlas    → Heimdall (CPU, 8K)",
    "",
    "If you switch to cloud models, remove or raise these limits."
  ],

  "agents": {
    "sisyphus": {
      "maxTokens": 32768
    },
    "hephaestus": {
      "maxTokens": 24576
    },
    "sisyphus-junior": {
      "maxTokens": 24576
    },
    "oracle": {
      "maxTokens": 16384
    },
    "prometheus": {
      "maxTokens": 16384
    },
    "metis": {
      "maxTokens": 16384
    },
    "momus": {
      "maxTokens": 16384
    },
    "librarian": {
      "maxTokens": 4096
    },
    "explore": {
      "maxTokens": 4096
    },
    "atlas": {
      "maxTokens": 4096
    }
  }
}
