# ===== HuggingFace =====
HF_TOKEN=
HF_HOME=${HOME}/.cache/huggingface
VLLM_CACHE=${HOME}/.cache/vllm
LLAMA_MODELS_DIR=./models

# ===== Ports =====
ORCH_PORT=8001
FAST_PORT=8004
LLAMA_ORCH_PORT=8011

# ===== vLLM models =====
# Shared instance serves both orchestrator and coder roles
ORCH_MODEL=Qwen/Qwen3-Coder-30B-A3B-Instruct
# ORCH_MODEL=meta-llama/Llama-3.3-70B-Instruct
FAST_MODEL=Qwen/Qwen2.5-7B-Instruct

# ===== vLLM tuning (single user; fractions must sum to ≤ 1.0 for shared GPU) =====
# Two services: orch (shared orch+coder) + fast. Leaves ~15% (~19 GB) headroom for system.
ORCH_GPU_UTIL=0.65
FAST_GPU_UTIL=0.20
# Context windows: Qwen3-Coder-30B supports 256K native; 32K is practical for coding.
# KV cache: ~3 GB of ~23 GB budget at 32K (96 KB/token × 32K tokens).
# With FP8 KV cache: ~1.5 GB (halved), leaving more room for concurrent sequences.
ORCH_MAX_LEN=32768
# Qwen2.5-7B: 8K output cap; 8K context sufficient for utility tasks.
# KV cache: ~0.4 GB of ~11.6 GB budget at 8K (56 KB/token × 8K tokens).
# With FP8 KV: ~0.2 GB (halved).
FAST_MAX_LEN=8192

# ===== vLLM optimization flags =====
# FP8 KV cache halves KV memory per token (confirmed supported on ROCm/gfx1151 in vLLM 0.16).
# Reduces 96 KB/token → 48 KB/token for orch, 56 KB/token → 28 KB/token for fast.
ORCH_KV_CACHE_DTYPE=fp8
FAST_KV_CACHE_DTYPE=fp8
# Limit concurrent sequences — single-user doesn't need high parallelism.
# Lower values = less pre-allocated activation memory overhead.
ORCH_MAX_NUM_SEQS=4
FAST_MAX_NUM_SEQS=4
# Weight quantization: NONE available on Strix Halo gfx1151 (RDNA 3.5) with vLLM 0.16.
#   FP8 (--quantization fp8): torch._scaled_mm requires MI300+ or CUDA sm_89+.
#   AWQ INT4 (pre-quantized checkpoint): Triton AWQ kernels → GPU memory fault + hang.
#   GPTQ/Marlin/bitsandbytes: CUDA-only in vLLM.
# Only FP8 KV cache (--kv-cache-dtype fp8) works — it's just data packing, no compute.
# All models must use BF16 weights on this platform.
# If future vLLM/ROCm updates add RDNA support, uncomment:
# ORCH_QUANTIZATION=fp8
# FAST_QUANTIZATION=fp8

# ===== llama.cpp fallback orchestrator (GGUF file inside ./models) =====
ORCH_GGUF=Qwen3-30B-A3B-Instruct.Q4_K_M.gguf
# ORCH_GGUF=llama-3.3-70b-instruct.Q4_K_M.gguf

# ===== Cloud planner (optional) =====
# OPENAI_API_KEY=...
# CLOUD_PLANNER_MODEL=gpt-5
