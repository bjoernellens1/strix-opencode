# ===== HuggingFace =====
HF_TOKEN=
HF_HOME=${HOME}/.cache/huggingface
VLLM_CACHE=${HOME}/.cache/vllm

# =============================================================================
#  Norse Agent Architecture — Strix Halo (128 GB UMA, gfx1151)
#
#  Phase 5: Hybrid Architecture (llama.cpp + vLLM)
#
#  Main agents (llama.cpp, GGUF Q4_K_M, ROCm GPU):
#    Thor: Qwen2.5-14B-Instruct (~8 GB) — always running
#    Valkyrie: Qwen3-Coder-30B-A3B-Instruct (~17 GB) — standard profile
#    Odin: Llama-3.3-70B-Instruct (~40 GB) — on-demand, stops Valkyrie
#
#  Utility agents (vLLM, BF16, on-demand via Bifrost):
#    Heimdall: Qwen2.5-3B-Instruct (~6 GB)
#    Loki: Qwen2.5-7B-Instruct (~14 GB)
#    Frigga: Qwen2.5-14B-Instruct (~28 GB)
#
#  Memory budget (120 GB usable):
#    Thor (8) + Valkyrie (17) + utility (max 28) = ~53 GB
#    Thor (8) + Valkyrie (17) + Odin (40) = ~65 GB
#
#  Legacy modes available: compose/vllm.yml, compose/cpu.yml, compose/gpu-all.yml
# =============================================================================

# ===== Ports =====
THOR_PORT=8001
VALKYRIE_PORT=8002
ODIN_PORT=8011
HEIMDALL_PORT=8012
LOKI_PORT=8013
FRIGGA_PORT=8014
BIFROST_PORT=8899

# ===== Models =====
# Thor — plans, coordinates, delegates. Long context, strong instruction following.
THOR_MODEL=Qwen/Qwen2.5-14B-Instruct
# Valkyrie — writes code, tool use. Purpose-built coding model.
VALKYRIE_MODEL=Qwen/Qwen3-Coder-30B-A3B-Instruct
# Odin — deep reasoning, architecture review, security audit. Thinking model.
ODIN_MODEL=Qwen/QwQ-32B
# Heimdall — fast validation, monitoring, simple checks.
HEIMDALL_MODEL=Qwen/Qwen2.5-3B-Instruct
# Loki — adversarial testing, edge cases, creative challenges.
LOKI_MODEL=Qwen/Qwen2.5-7B-Instruct
# Frigga — documentation, context compression, knowledge curation.
FRIGGA_MODEL=Qwen/Qwen2.5-14B-Instruct

# ===== GPU Memory Fractions (BF16, must sum to ≤ 1.0 per active profile) =====
#
# Profile "standard": Thor (0.35) + Valkyrie (0.60) = 0.95
# Profile "odin":     Thor (0.35) + Odin (0.55) = 0.90
# Profile "heimdall": Thor (0.35) + Heimdall (0.05) = 0.40
# Profile "loki":     Thor (0.35) + Loki (0.12) = 0.47
# Profile "frigga":   Thor (0.35) + Frigga (0.25) = 0.60

# Tier 0: Core (always running)
THOR_GPU_UTIL=0.35

# Tier 1: Standard
VALKYRIE_GPU_UTIL=0.60

# Tier 2: On-demand utility
HEIMDALL_GPU_UTIL=0.05
LOKI_GPU_UTIL=0.12
FRIGGA_GPU_UTIL=0.25

# Tier 3: Escalation
ODIN_GPU_UTIL=0.55

# ===== Context Windows (BF16 KV cache — no FP8 on this hardware) =====
#
# Thor (Qwen2.5-14B-Instruct):
#   Weights: ~28 GB BF16
#   Budget: 0.35 × 128 GB = 44.8 GB → 16.8 GB for KV
#   KV per token: 40 layers × 2 × 8 KV heads × 128 dim × 2 bytes = 163,840 B ≈ 160 KB
#   64K tokens × 160 KB = 10 GB → fits in 16.8 GB budget (6.8 GB slack)
THOR_MAX_LEN=65536

# Valkyrie (Qwen3-Coder-30B-A3B-Instruct):
#   Weights: ~57 GB BF16 (MoE: 30.5B total params, 3.3B active per token)
#   Budget: 0.60 × 128 GB = 76.8 GB → 19.8 GB for KV
#   KV per token: 48 layers × 2 × 4 KV heads × 128 dim × 2 bytes = 98,304 B ≈ 96 KB
#   48K tokens × 96 KB = 4.5 GB → fits in 19.8 GB budget (15.3 GB slack)
VALKYRIE_MAX_LEN=49152

# Odin (QwQ-32B):
#   Weights: ~65 GB BF16
#   Budget: 0.55 × 128 GB = 70.4 GB → 5.4 GB for KV
#   KV per token: 64 layers × 2 × 8 KV heads × 128 dim × 2 bytes = 262,144 B ≈ 256 KB
#   20K tokens × 256 KB = 5.0 GB → fits in 5.4 GB budget (0.4 GB slack)
#   Note: QwQ-32B supports 131K natively but we cap at 20K for memory.
ODIN_MAX_LEN=20480

# Heimdall (Qwen2.5-3B-Instruct):
#   Weights: ~6 GB BF16. Tiny model, generous KV budget.
HEIMDALL_MAX_LEN=8192

# Loki (Qwen2.5-7B-Instruct):
#   Weights: ~14 GB BF16.
LOKI_MAX_LEN=16384

# Frigga (Qwen2.5-14B-Instruct):
#   Weights: ~28 GB BF16.
#   Budget: 0.25 × 128 GB = 32 GB → 4 GB for KV
FRIGGA_MAX_LEN=16384

# KV cache dtype: MUST be auto (BF16) on Strix Halo gfx1151.
THOR_KV_CACHE_DTYPE=auto
VALKYRIE_KV_CACHE_DTYPE=auto

# Max concurrent sequences (single-user, low is better for memory)
THOR_MAX_NUM_SEQS=4
VALKYRIE_MAX_NUM_SEQS=4
ODIN_MAX_NUM_SEQS=2
HEIMDALL_MAX_NUM_SEQS=4
LOKI_MAX_NUM_SEQS=4
FRIGGA_MAX_NUM_SEQS=4

# ===== Legacy CPU agents (compose/cpu.yml — fallback mode) =====
LLAMA_MODELS_DIR=./models

# ===== llama.cpp GGUF models (compose/hybrid.yml — Phase 5) =====
# Download from HuggingFace:
#   Thor: huggingface-cli download unsloth/Qwen2.5-14B-Instruct-GGUF qwen2.5-14b-instruct-q4_k_m.gguf --local-dir models
#   Valkyrie: huggingface-cli download unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf --local-dir models
#   Odin: huggingface-cli download bartowski/Llama-3.3-70B-Instruct-GGUF llama-3.3-70b-instruct-Q4_K_M.gguf --local-dir models

THOR_GGUF=qwen2.5-14b-instruct-q4_k_m.gguf
THOR_CTX=65536
THOR_THREADS=8

VALKYRIE_GGUF=Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
VALKYRIE_CTX=49152
VALKYRIE_THREADS=8

ODIN_GGUF=llama-3.3-70b-instruct-Q4_K_M.gguf
ODIN_CTX=32768
ODIN_THREADS=16

# Legacy CPU-only (compose/cpu.yml)
ODIN_CPU_CTX=32768
HEIMDALL_GGUF=qwen2.5-3b-instruct.Q4_K_M.gguf
HEIMDALL_CPU_CTX=8192
HEIMDALL_THREADS=8
LOKI_GGUF=qwen2.5-7b-instruct.Q4_K_M.gguf
LOKI_CPU_CTX=16384
LOKI_THREADS=8
FRIGGA_GGUF=qwen2.5-14b-instruct-q4_k_m.gguf
FRIGGA_CPU_CTX=32768
FRIGGA_THREADS=12

# ===== Bifrost Scheduler =====
# BIFROST_PORT defined above (8899)
BIFROST_ODIN_TTL=1200
BIFROST_ODIN_COOLDOWN=600
BIFROST_SESSION_BUDGET_ODIN=3
BIFROST_SESSION_BUDGET_UTILITY=10

# ===== Cloud planner (optional) =====
# OPENAI_API_KEY=...
# CLOUD_PLANNER_MODEL=gpt-5
