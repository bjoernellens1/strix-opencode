# ===== HuggingFace =====
HF_TOKEN=your_huggingface_token_here
HF_HOME=/home/username/.cache/huggingface
VLLM_CACHE=/home/username/.cache/vllm

# ===== Ollama =====
# Shared model store for all Ollama containers.
OLLAMA_MODELS=/home/username/.ollama
# Concurrency per model instance. Memory grows with context * parallelism.
OLLAMA_NUM_PARALLEL=5
# Keep multiple models loaded per Ollama daemon.
OLLAMA_MAX_LOADED_MODELS=3
# How long Ollama keeps a model warm in memory.
OLLAMA_KEEP_ALIVE=5m
# Single Ollama port (OpenAI-compatible API).
OLLAMA_PORT=11434
# Base image for the Ollama container build (Strix Halo toolboxes).
OLLAMA_TOOLBOX_IMAGE=docker.io/kyuz0/amd-strix-halo-toolboxes:rocm-7.1-rocwmma

# =============================================================================
#  Norse Agent Architecture — Strix Halo (128 GB UMA, gfx1151)
#  Phase 6: Full vLLM AWQ Architecture
# =============================================================================

# ===== Ports (legacy vLLM/llama.cpp) =====
THOR_PORT=8001
VALKYRIE_PORT=8002
ODIN_PORT=8011
HEIMDALL_PORT=8012
LOKI_PORT=8013
FRIGGA_PORT=8014
BIFROST_PORT=8899

# ===== Models (vLLM AWQ) =====
# Thor — plans, coordinates, delegates. Long context, strong instruction following.
THOR_MODEL=Qwen/Qwen2.5-14B-Instruct-AWQ
# Valkyrie — writes code, tool use. Purpose-built coding model.
VALKYRIE_MODEL=QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ
# Odin — deep reasoning, architecture review.
ODIN_MODEL=casperhansen/llama-3.3-70b-instruct-awq
# Utility Models
HEIMDALL_MODEL=Qwen/Qwen2.5-3B-Instruct-AWQ
LOKI_MODEL=Qwen/Qwen2.5-7B-Instruct-AWQ
FRIGGA_MODEL=Qwen/Qwen2.5-14B-Instruct-AWQ

# ===== Models (Ollama tags) =====
# These should match `ollama list` output on your machine.
THOR_OLLAMA_MODEL=qwen2.5:14b-instruct-q4_K_M-64k
VALKYRIE_OLLAMA_MODEL=qwen3-coder:30b-64k
ODIN_OLLAMA_MODEL=llama3:70b-instruct-q4_K_M-64k
HEIMDALL_OLLAMA_MODEL=qwen2.5:3b-instruct
LOKI_OLLAMA_MODEL=qwen2.5:7b-instruct
FRIGGA_OLLAMA_MODEL=qwen2.5:14b-instruct-q4_K_M-64k

# ===== GPU Memory Fractions (AWQ 4-bit) =====
# Tier 0: Core
THOR_GPU_UTIL=0.20
# Tier 1: Standard
VALKYRIE_GPU_UTIL=0.40
# Tier 2: Utility
HEIMDALL_GPU_UTIL=0.08
LOKI_GPU_UTIL=0.12
FRIGGA_GPU_UTIL=0.15
# Tier 3: Escalation
ODIN_GPU_UTIL=0.50

# ===== Context Windows =====
THOR_MAX_LEN=32768
VALKYRIE_MAX_LEN=32768
ODIN_MAX_LEN=32768
HEIMDALL_MAX_LEN=32768
LOKI_MAX_LEN=32768
FRIGGA_MAX_LEN=32768

# ===== Max Concurrent Sequences =====
THOR_MAX_NUM_SEQS=4
VALKYRIE_MAX_NUM_SEQS=4
ODIN_MAX_NUM_SEQS=4
HEIMDALL_MAX_NUM_SEQS=4
LOKI_MAX_NUM_SEQS=4
FRIGGA_MAX_NUM_SEQS=4

# ===== vLLM Settings =====
THOR_KV_CACHE_DTYPE=auto
VALKYRIE_KV_CACHE_DTYPE=auto

# ===== Bifrost Scheduler =====
BIFROST_ODIN_TTL=1200
BIFROST_ODIN_COOLDOWN=600
BIFROST_SESSION_BUDGET_ODIN=3
BIFROST_SESSION_BUDGET_UTILITY=10

# ===== Cloud planner (optional) =====
# OPENAI_API_KEY=your_openai_api_key_here
# CLOUD_PLANNER_MODEL=gpt-5

# ===== Infrastructure =====
# REQUIRED for Bifrost: Absolute path to this repository
REPO_ROOT=/home/username/git/strix-opencode
