# ===== HuggingFace =====
HF_TOKEN=your_huggingface_token_here
HF_HOME=/home/username/.cache/huggingface
VLLM_CACHE=/home/username/.cache/vllm

# =============================================================================
#  Norse Agent Architecture — Strix Halo (128 GB UMA, gfx1151)
#  Phase 6: Full vLLM + AWQ Architecture
# =============================================================================

# ===== Ports =====
THOR_PORT=8001
VALKYRIE_PORT=8002
ODIN_PORT=8011
HEIMDALL_PORT=8012
LOKI_PORT=8013
FRIGGA_PORT=8014
BIFROST_PORT=8899

# ===== Models (vLLM AWQ) =====
# Thor — plans, coordinates, delegates. Long context, strong instruction following.
THOR_MODEL=Qwen/Qwen2.5-14B-Instruct-AWQ
# Valkyrie — writes code, tool use. Purpose-built coding model.
VALKYRIE_MODEL=QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ
# Odin — deep reasoning, architecture review.
ODIN_MODEL=casperhansen/llama-3.3-70b-instruct-awq
# Utility Models
HEIMDALL_MODEL=Qwen/Qwen2.5-3B-Instruct-AWQ
LOKI_MODEL=Qwen/Qwen2.5-7B-Instruct-AWQ
FRIGGA_MODEL=Qwen/Qwen2.5-14B-Instruct-AWQ

# ===== GPU Memory Fractions (AWQ 4-bit) =====
# Tier 0: Core
THOR_GPU_UTIL=0.20
# Tier 1: Standard
VALKYRIE_GPU_UTIL=0.40
# Tier 2: Utility
HEIMDALL_GPU_UTIL=0.08
LOKI_GPU_UTIL=0.12
FRIGGA_GPU_UTIL=0.15
# Tier 3: Escalation
ODIN_GPU_UTIL=0.50

# ===== Context Windows =====
THOR_MAX_LEN=32768
VALKYRIE_MAX_LEN=32768
ODIN_MAX_LEN=32768
HEIMDALL_MAX_LEN=32768
LOKI_MAX_LEN=32768
FRIGGA_MAX_LEN=32768

# ===== Max Concurrent Sequences =====
THOR_MAX_NUM_SEQS=4
VALKYRIE_MAX_NUM_SEQS=4
ODIN_MAX_NUM_SEQS=4
HEIMDALL_MAX_NUM_SEQS=4
LOKI_MAX_NUM_SEQS=4
FRIGGA_MAX_NUM_SEQS=4

# ===== vLLM Settings =====
THOR_KV_CACHE_DTYPE=auto
VALKYRIE_KV_CACHE_DTYPE=auto

# ===== Bifrost Scheduler =====
BIFROST_ODIN_TTL=1200
BIFROST_ODIN_COOLDOWN=600
BIFROST_SESSION_BUDGET_ODIN=3
BIFROST_SESSION_BUDGET_UTILITY=10

# ===== Infrastructure =====
# REQUIRED for Bifrost: Absolute path to this repository
REPO_ROOT=/home/username/git/strix-opencode
