# ===== Paths =====
HF_HOME=${HOME}/.cache/huggingface
VLLM_CACHE=${HOME}/.cache/vllm
LLAMA_MODELS_DIR=./models

# ===== Ports =====
ORCH_PORT=8001
CODER_PORT=8002
FAST_PORT=8004
LLAMA_ORCH_PORT=8011

# ===== vLLM models =====
ORCH_MODEL=openai/gpt-oss-120b
# ORCH_MODEL=meta-llama/Llama-3.3-70B-Instruct
CODER_MODEL=Qwen/Qwen3-Coder-30B-A3B-Instruct
FAST_MODEL=Qwen/Qwen2.5-7B-Instruct

# ===== vLLM tuning (single user; keep conservative) =====
ORCH_GPU_UTIL=0.85
CODER_GPU_UTIL=0.80
FAST_GPU_UTIL=0.70
ORCH_MAX_LEN=8192
CODER_MAX_LEN=8192
FAST_MAX_LEN=4096

# ===== llama.cpp fallback orchestrator (GGUF file inside ./models) =====
ORCH_GGUF=gpt-oss-120b.Q4_K_M.gguf
# ORCH_GGUF=llama-3.3-70b-instruct.Q4_K_M.gguf

# ===== Cloud planner (optional) =====
# OPENAI_API_KEY=...
# CLOUD_PLANNER_MODEL=gpt-5
