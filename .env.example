# ===== HuggingFace =====
HF_TOKEN=
HF_HOME=${HOME}/.cache/huggingface
VLLM_CACHE=${HOME}/.cache/vllm
LLAMA_MODELS_DIR=./models

# =============================================================================
#  Norse Agent Architecture — Strix Halo (128 GB UMA, gfx1151)
#
#  GPU agents (vLLM, BF16 only — no quantization works on RDNA 3.5):
#    Thor      — Primary Commander / Orchestrator (14B, GPU)
#    Valkyrie  — Execution Specialist / Coder (30B MoE, GPU)
#
#  CPU agents (llama.cpp, GGUF INT4):
#    Odin      — Supreme Architect / Escalation Reviewer (70B, CPU)
#    Heimdall  — Guardian / Monitoring & Validation (3B, CPU)
#    Loki      — Adversarial / Creative Intelligence (8B, CPU)
#    Frigga    — Knowledge / Context Memory Curator (14B, CPU)
# =============================================================================

# ===== Ports =====
# GPU agents
THOR_PORT=8001
VALKYRIE_PORT=8002
# CPU agents
ODIN_PORT=8011
HEIMDALL_PORT=8012
LOKI_PORT=8013
FRIGGA_PORT=8014

# ===== vLLM models (GPU agents) =====
# Thor — plans, coordinates, delegates. Long context, strong instruction following.
THOR_MODEL=Qwen/Qwen2.5-14B-Instruct
# Valkyrie — writes code, tool use. Purpose-built coding model.
VALKYRIE_MODEL=Qwen/Qwen3-Coder-30B-A3B-Instruct

# ===== vLLM tuning (single user; fractions must sum to ≤ 1.0 for shared GPU) =====
# Valkyrie gets the lion's share (0.60) — biggest model (~57 GB BF16).
# Thor is smaller (14B, ~28 GB BF16) and gets 0.35.
# Total: 0.95 — leaves ~6.4 GB for system. Tight but workable for single-user.
THOR_GPU_UTIL=0.35
VALKYRIE_GPU_UTIL=0.60

# Context windows (BF16 KV cache — no FP8 on this hardware):
#
# Thor (Qwen2.5-14B-Instruct):
#   Weights: ~28 GB BF16
#   Budget: 0.35 × 128 GB = 44.8 GB → 16.8 GB for KV
#   KV per token: 40 layers × 2 × 8 KV heads × 128 dim × 2 bytes = 163,840 B ≈ 160 KB
#   64K tokens × 160 KB = 10 GB → fits in 16.8 GB budget (6.8 GB slack)
THOR_MAX_LEN=65536

# Valkyrie (Qwen3-Coder-30B-A3B-Instruct):
#   Weights: ~57 GB BF16 (MoE: 30.5B total params, 3.3B active per token)
#   Budget: 0.60 × 128 GB = 76.8 GB → 19.8 GB for KV
#   KV per token: 48 layers × 2 × 4 KV heads × 128 dim × 2 bytes = 98,304 B ≈ 96 KB
#   48K tokens × 96 KB = 4.5 GB → fits in 19.8 GB budget (15.3 GB slack)
VALKYRIE_MAX_LEN=49152

# KV cache dtype: MUST be auto (BF16) on Strix Halo gfx1151.
# FP8 KV cache "works" in software but uses uncalibrated scales — accuracy loss.
THOR_KV_CACHE_DTYPE=auto
VALKYRIE_KV_CACHE_DTYPE=auto

# Limit concurrent sequences — single-user doesn't need high parallelism.
THOR_MAX_NUM_SEQS=4
VALKYRIE_MAX_NUM_SEQS=4

# Weight quantization: NONE available on Strix Halo gfx1151 (RDNA 3.5) with vLLM 0.16.
#   FP8 (--quantization fp8): torch._scaled_mm requires MI300+ or CUDA sm_89+.
#   AWQ INT4 (pre-quantized checkpoint): Triton AWQ kernels → GPU memory fault + hang.
#   GPTQ/Marlin/bitsandbytes: CUDA-only in vLLM.
# ALL models must use BF16 weights + BF16 KV cache on this platform.

# ===== llama.cpp CPU agents (GGUF files inside ./models) =====
# Odin — Supreme Architect. 70B for deep reasoning, architecture review, escalation.
ODIN_GGUF=llama-3.3-70b-instruct.Q4_K_M.gguf
ODIN_CTX=32768
ODIN_THREADS=16

# Heimdall — Guardian. 3B model for fast validation, monitoring, simple checks.
HEIMDALL_GGUF=qwen2.5-3b-instruct.Q4_K_M.gguf
HEIMDALL_CTX=8192
HEIMDALL_THREADS=8

# Loki — Adversarial Intelligence. 8B model for edge cases, chaos testing, alternatives.
LOKI_GGUF=qwen2.5-7b-instruct.Q4_K_M.gguf
LOKI_CTX=16384
LOKI_THREADS=8

# Frigga — Knowledge Curator. 14B model for documentation, context compression, memory.
FRIGGA_GGUF=qwen2.5-14b-instruct.Q4_K_M.gguf
FRIGGA_CTX=32768
FRIGGA_THREADS=12

# ===== Cloud planner (optional) =====
# OPENAI_API_KEY=...
# CLOUD_PLANNER_MODEL=gpt-5
