# ===== HuggingFace =====
HF_TOKEN=
HF_HOME=${HOME}/.cache/huggingface
VLLM_CACHE=${HOME}/.cache/vllm
LLAMA_MODELS_DIR=./models

# ===== Ports =====
# Tier 1: GPU Orchestrator (vLLM)
ORCH_PORT=8001
# Tier 2: GPU Coder (vLLM)
CODER_PORT=8002
# Tier 3: CPU Escalation Reviewer (llama.cpp)
REVIEWER_PORT=8011
# Tier 0: CPU Utility Agent (llama.cpp)
UTILITY_PORT=8012

# ===== vLLM models (GPU tiers) =====
# Tier 1: Orchestrator — plans, coordinates, delegates. Long context, moderate speed.
ORCH_MODEL=Qwen/Qwen2.5-14B-Instruct
# Tier 2: Coder — writes code, tool use. Medium context, needs large model for quality.
CODER_MODEL=Qwen/Qwen3-Coder-30B-A3B-Instruct

# ===== vLLM tuning (single user; fractions must sum to ≤ 1.0 for shared GPU) =====
# Architecture: Coder gets the lion's share (0.60) because it's the big model (~57 GB BF16).
# Orchestrator is smaller (14B, ~28 GB BF16) and gets 0.35.
# Total: 0.95 — leaves ~6.4 GB for system. Tight but workable for single-user.
ORCH_GPU_UTIL=0.35
CODER_GPU_UTIL=0.60

# Context windows (BF16 KV cache — no FP8 on this hardware):
#
# Orchestrator (Qwen2.5-14B-Instruct):
#   Weights: ~28 GB BF16
#   Budget: 0.35 × 128 GB = 44.8 GB → 16.8 GB for KV
#   KV per token: 40 layers × 2 × 8 KV heads × 128 dim × 2 bytes = 163,840 B ≈ 160 KB
#   64K tokens × 160 KB = 10 GB → fits in 16.8 GB budget (6.8 GB slack)
ORCH_MAX_LEN=65536

# Coder (Qwen3-Coder-30B-A3B-Instruct):
#   Weights: ~57 GB BF16 (MoE: 30.5B total params, 3.3B active per token)
#   Budget: 0.60 × 128 GB = 76.8 GB → 19.8 GB for KV
#   KV per token: 48 layers × 2 × 4 KV heads × 128 dim × 2 bytes = 98,304 B ≈ 96 KB
#   48K tokens × 96 KB = 4.5 GB → fits in 19.8 GB budget (15.3 GB slack)
CODER_MAX_LEN=49152

# KV cache dtype: MUST be auto (BF16) on Strix Halo gfx1151.
# FP8 KV cache "works" in software but uses uncalibrated scales (q_scale=1.0, prob_scale=1.0),
# causing accuracy issues. No hardware FP8 support on RDNA 3.5.
ORCH_KV_CACHE_DTYPE=auto
CODER_KV_CACHE_DTYPE=auto

# Limit concurrent sequences — single-user doesn't need high parallelism.
ORCH_MAX_NUM_SEQS=4
CODER_MAX_NUM_SEQS=4

# Weight quantization: NONE available on Strix Halo gfx1151 (RDNA 3.5) with vLLM 0.16.
#   FP8 (--quantization fp8): torch._scaled_mm requires MI300+ or CUDA sm_89+.
#   AWQ INT4 (pre-quantized checkpoint): Triton AWQ kernels → GPU memory fault + hang.
#   GPTQ/Marlin/bitsandbytes: CUDA-only in vLLM.
#   FP8 KV cache (--kv-cache-dtype fp8): Software emulation only, uncalibrated — accuracy loss.
# ALL models must use BF16 weights + BF16 KV cache on this platform.

# ===== llama.cpp CPU tiers (GGUF files inside ./models) =====
# Tier 3: Escalation Reviewer — 70B INT4 GGUF for deep review tasks. CPU-only.
REVIEWER_GGUF=llama-3.3-70b-instruct.Q4_K_M.gguf
REVIEWER_CTX=32768
REVIEWER_THREADS=16

# Tier 0: Utility Agent — 3B model for fast utility tasks. CPU-only.
UTILITY_GGUF=qwen2.5-3b-instruct.Q4_K_M.gguf
UTILITY_CTX=8192
UTILITY_THREADS=8

# ===== Cloud planner (optional) =====
# OPENAI_API_KEY=...
# CLOUD_PLANNER_MODEL=gpt-5
