# GPU-Only Norse Agent Architecture — Bifrost Scheduler Mode
#
# All 6 agents run on GPU via vLLM (BF16 only on Strix Halo gfx1151).
# Thor + Valkyrie are the "standard" profile (always-on during normal operation).
# Heimdall, Loki, Frigga, Odin are on-demand — managed by the Bifrost scheduler.
#
# Bifrost handles profile switching:
#   - "standard": Thor (0.35) + Valkyrie (0.60) = 0.95
#   - "heimdall": Thor (0.35) + Heimdall (0.05) = 0.40 (stops Valkyrie)
#   - "loki":     Thor (0.35) + Loki (0.12) = 0.47 (stops Valkyrie)
#   - "frigga":   Thor (0.35) + Frigga (0.25) = 0.60 (stops Valkyrie)
#   - "odin":     Thor (0.35) + Odin (0.55) = 0.90 (stops Valkyrie)
#
# IMPORTANT: Only ONE on-demand agent can run at a time alongside Thor.
# Valkyrie and on-demand agents are mutually exclusive (shared GPU memory).
#
# Staggered startup: Thor loads first (healthcheck gate), then the next agent.
# On shared-memory GPUs (Strix Halo), concurrent vLLM startup causes the
# memory profiler to see the other instance's allocation, leading to crash.
#
# All BF16 — no quantization or FP8 KV works on gfx1151 (RDNA 3.5).

services:
  # ====================================================================
  # TIER 0: CORE — Always running
  # ====================================================================

  vllm_thor:
    image: docker.io/kyuz0/vllm-therock-gfx1151:latest
    container_name: vllm_thor
    network_mode: host
    devices: ["/dev/kfd", "/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video", "render"]
    environment:
      - HF_HOME=${HF_HOME}
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HF_HOME}:${HF_HOME}
      - ${VLLM_CACHE}:${VLLM_CACHE}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:${THOR_PORT}/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    command: >
      vllm serve ${THOR_MODEL}
      --host 0.0.0.0 --port ${THOR_PORT}
      --dtype auto
      --gpu-memory-utilization ${THOR_GPU_UTIL}
      --max-model-len ${THOR_MAX_LEN}
      --kv-cache-dtype ${THOR_KV_CACHE_DTYPE:-auto}
      --max-num-seqs ${THOR_MAX_NUM_SEQS:-4}
      --enable-prefix-caching

  # ====================================================================
  # TIER 1: STANDARD — Default coding workflow (mutually exclusive with Tier 2/3)
  # ====================================================================

  vllm_valkyrie:
    image: docker.io/kyuz0/vllm-therock-gfx1151:latest
    container_name: vllm_valkyrie
    network_mode: host
    devices: ["/dev/kfd", "/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video", "render"]
    profiles: ["standard"]
    environment:
      - HF_HOME=${HF_HOME}
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HF_HOME}:${HF_HOME}
      - ${VLLM_CACHE}:${VLLM_CACHE}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:${VALKYRIE_PORT}/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    depends_on:
      vllm_thor:
        condition: service_healthy
    command: >
      vllm serve ${VALKYRIE_MODEL}
      --host 0.0.0.0 --port ${VALKYRIE_PORT}
      --dtype auto
      --gpu-memory-utilization ${VALKYRIE_GPU_UTIL}
      --max-model-len ${VALKYRIE_MAX_LEN}
      --kv-cache-dtype ${VALKYRIE_KV_CACHE_DTYPE:-auto}
      --max-num-seqs ${VALKYRIE_MAX_NUM_SEQS:-4}
      --enable-prefix-caching

  # ====================================================================
  # TIER 2: ON-DEMAND UTILITY — Managed by Bifrost (one at a time)
  # ====================================================================

  vllm_heimdall:
    image: docker.io/kyuz0/vllm-therock-gfx1151:latest
    container_name: vllm_heimdall
    network_mode: host
    devices: ["/dev/kfd", "/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video", "render"]
    profiles: ["heimdall"]
    environment:
      - HF_HOME=${HF_HOME}
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HF_HOME}:${HF_HOME}
      - ${VLLM_CACHE}:${VLLM_CACHE}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:${HEIMDALL_PORT}/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    depends_on:
      vllm_thor:
        condition: service_healthy
    command: >
      vllm serve ${HEIMDALL_MODEL}
      --host 0.0.0.0 --port ${HEIMDALL_PORT}
      --dtype auto
      --gpu-memory-utilization ${HEIMDALL_GPU_UTIL}
      --max-model-len ${HEIMDALL_MAX_LEN}
      --kv-cache-dtype auto
      --max-num-seqs ${HEIMDALL_MAX_NUM_SEQS:-4}
      --enable-prefix-caching

  vllm_loki:
    image: docker.io/kyuz0/vllm-therock-gfx1151:latest
    container_name: vllm_loki
    network_mode: host
    devices: ["/dev/kfd", "/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video", "render"]
    profiles: ["loki"]
    environment:
      - HF_HOME=${HF_HOME}
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HF_HOME}:${HF_HOME}
      - ${VLLM_CACHE}:${VLLM_CACHE}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:${LOKI_PORT}/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    depends_on:
      vllm_thor:
        condition: service_healthy
    command: >
      vllm serve ${LOKI_MODEL}
      --host 0.0.0.0 --port ${LOKI_PORT}
      --dtype auto
      --gpu-memory-utilization ${LOKI_GPU_UTIL}
      --max-model-len ${LOKI_MAX_LEN}
      --kv-cache-dtype auto
      --max-num-seqs ${LOKI_MAX_NUM_SEQS:-4}
      --enable-prefix-caching

  vllm_frigga:
    image: docker.io/kyuz0/vllm-therock-gfx1151:latest
    container_name: vllm_frigga
    network_mode: host
    devices: ["/dev/kfd", "/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video", "render"]
    profiles: ["frigga"]
    environment:
      - HF_HOME=${HF_HOME}
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HF_HOME}:${HF_HOME}
      - ${VLLM_CACHE}:${VLLM_CACHE}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:${FRIGGA_PORT}/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    depends_on:
      vllm_thor:
        condition: service_healthy
    command: >
      vllm serve ${FRIGGA_MODEL}
      --host 0.0.0.0 --port ${FRIGGA_PORT}
      --dtype auto
      --gpu-memory-utilization ${FRIGGA_GPU_UTIL}
      --max-model-len ${FRIGGA_MAX_LEN}
      --kv-cache-dtype auto
      --max-num-seqs ${FRIGGA_MAX_NUM_SEQS:-4}
      --enable-prefix-caching

  # ====================================================================
  # TIER 3: ESCALATION — Odin (QwQ-32B reasoning model)
  # Requires stopping Valkyrie to free GPU memory.
  # ====================================================================

  vllm_odin:
    image: docker.io/kyuz0/vllm-therock-gfx1151:latest
    container_name: vllm_odin
    network_mode: host
    devices: ["/dev/kfd", "/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video", "render"]
    profiles: ["odin"]
    environment:
      - HF_HOME=${HF_HOME}
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HF_HOME}:${HF_HOME}
      - ${VLLM_CACHE}:${VLLM_CACHE}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:${ODIN_PORT}/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 90
      start_period: 180s
    depends_on:
      vllm_thor:
        condition: service_healthy
    command: >
      vllm serve ${ODIN_MODEL}
      --host 0.0.0.0 --port ${ODIN_PORT}
      --dtype auto
      --gpu-memory-utilization ${ODIN_GPU_UTIL}
      --max-model-len ${ODIN_MAX_LEN}
      --kv-cache-dtype auto
      --max-num-seqs ${ODIN_MAX_NUM_SEQS:-2}
      --enable-prefix-caching
      --enable-reasoning
      --reasoning-parser deepseek_r1

  # ====================================================================
  # BIFROST — Scheduler daemon (FastAPI)
  # ====================================================================

  bifrost:
    build:
      context: ../bifrost
      dockerfile: Dockerfile
    container_name: bifrost
    network_mode: host
    environment:
      - BIFROST_PORT=${BIFROST_PORT:-8899}
      - COMPOSE_PROJECT_DIR=${COMPOSE_PROJECT_DIR:-/app}
      - DOCKER_HOST=${DOCKER_HOST:-unix:///var/run/docker.sock}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ..:/app:ro
    profiles: ["bifrost"]
    command: >
      uvicorn app:app --host 0.0.0.0 --port ${BIFROST_PORT:-8899}
