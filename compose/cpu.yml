# Odin ðŸ‘ï¸ (CPU): Supreme Architect / Escalation Reviewer â€” Llama-3.3-70B Q4_K_M GGUF
# Heimdall ðŸ‘ (CPU): Guardian / Monitoring & Validation â€” Qwen2.5-3B Q4_K_M GGUF
# Loki ðŸ§  (CPU): Adversarial / Creative Intelligence â€” Qwen2.5-7B Q4_K_M GGUF
# Frigga ðŸŒ¿ (CPU): Knowledge / Context Memory Curator â€” Qwen2.5-14B Q4_K_M GGUF
#
# All run on llama.cpp (CPU-only, no GPU). They share system RAM with the
# GPU tiers but only load on demand â€” not expected to run concurrently with
# both vLLM services at full KV pressure.
#
# GGUF files must be pre-downloaded into ./models/ before starting.

services:
  llama_odin:
    image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv
    container_name: llama_odin
    network_mode: host
    devices: ["/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video"]
    volumes:
      - ${LLAMA_MODELS_DIR}:/models
    command: >
      bash -lc "
      llama-server
      --host 0.0.0.0 --port ${ODIN_PORT}
      -m /models/${ODIN_GGUF}
      -c ${ODIN_CTX}
      -t ${ODIN_THREADS}
      --no-mmap
      "

  llama_heimdall:
    image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv
    container_name: llama_heimdall
    network_mode: host
    devices: ["/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video"]
    volumes:
      - ${LLAMA_MODELS_DIR}:/models
    command: >
      bash -lc "
      llama-server
      --host 0.0.0.0 --port ${HEIMDALL_PORT}
      -m /models/${HEIMDALL_GGUF}
      -c ${HEIMDALL_CTX}
      -t ${HEIMDALL_THREADS}
      --no-mmap
      "

  llama_loki:
    image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv
    container_name: llama_loki
    network_mode: host
    devices: ["/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video"]
    volumes:
      - ${LLAMA_MODELS_DIR}:/models
    command: >
      bash -lc "
      llama-server
      --host 0.0.0.0 --port ${LOKI_PORT}
      -m /models/${LOKI_GGUF}
      -c ${LOKI_CTX}
      -t ${LOKI_THREADS}
      --no-mmap
      "

  llama_frigga:
    image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv
    container_name: llama_frigga
    network_mode: host
    devices: ["/dev/dri"]
    security_opt: ["seccomp=unconfined"]
    group_add: ["video"]
    volumes:
      - ${LLAMA_MODELS_DIR}:/models
    command: >
      bash -lc "
      llama-server
      --host 0.0.0.0 --port ${FRIGGA_PORT}
      -m /models/${FRIGGA_GGUF}
      -c ${FRIGGA_CTX}
      -t ${FRIGGA_THREADS}
      --no-mmap
      "
